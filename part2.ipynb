{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2af521c",
      "metadata": {
        "id": "d2af521c"
      },
      "source": [
        "# Streamer Revisit Prediction on Twitch Data\n",
        "\n",
        "In this part we predict whether a user will revisit the same streamer in the near future.  \n",
        "The dataset consists of Twitch chat connections sampled every 10 minutes over 43 days.\n",
        "\n",
        "Each row has:\n",
        "- user_id  \n",
        "- stream_id  \n",
        "- streamer  \n",
        "- time_start  \n",
        "- time_stop  \n",
        "\n",
        "Durations are measured in 10-minute steps.\n",
        "\n",
        "Basic dataset characteristics:\n",
        "- 100k users  \n",
        "- 162.6k streamers  \n",
        "- ~3M interactions  \n",
        "- 6148 total time steps  \n",
        "\n",
        "We formulate revisit prediction as a binary classification and ranking task.  \n",
        "For each interaction, we predict whether the user will return to the same streamer within a fixed time window.  \n",
        "This aligns with temporal recommender-style modeling and session behavior prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e61060e",
      "metadata": {
        "id": "1e61060e"
      },
      "source": [
        "## Imports and constants\n",
        "\n",
        "In this part we import required libraries and define constants that control the experiment setup.  \n",
        "We separate constants because they govern the behavior of the model and allow consistent reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfeadbe",
      "metadata": {
        "id": "7cfeadbe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "from IPython.display import display\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b0c95c",
      "metadata": {
        "id": "67b0c95c"
      },
      "outputs": [],
      "source": [
        "# Ensures reproducible random behavior throughout the notebook\n",
        "np.random.seed(77)\n",
        "\n",
        "# Eensures reproducible training\n",
        "RANDOM_STATE = 77\n",
        "\n",
        "# Defines the look-ahead window:\n",
        "# a user counts as a revisit if they return to the same streamer within this many time steps\n",
        "PREDICTION_WINDOW_STEPS = 12\n",
        "\n",
        "# Determines how much of the data is used as the test set (time based here)\n",
        "TEST_FRACTION = 0.2\n",
        "\n",
        "# Controls ranking evaluation:\n",
        "# precision@10 and recall@10 are computed using the top 10 predicted streamers per user\n",
        "TOP_K = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe3fc62",
      "metadata": {
        "id": "fbe3fc62"
      },
      "source": [
        "## Load data and basic preprocessing\n",
        "\n",
        "In this part we load the raw Twitch dataset and prepare it for modeling.\n",
        "\n",
        "We:\n",
        "- assign column names,\n",
        "- compute duration as `time_stop - time_start`,\n",
        "- clip it to at least one time step so all durations are positive,\n",
        "- sort interactions per user by time to enforce temporal order.\n",
        "\n",
        "We do this because all later feature engineering and label creation depend on chronological user histories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95c1eae",
      "metadata": {
        "id": "e95c1eae"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"100k_a.csv\", header=None)\n",
        "df.columns = [\"user_id\", \"stream_id\", \"streamer\", \"time_start\", \"time_stop\"]\n",
        "\n",
        "df[\"duration_steps\"] = (df[\"time_stop\"] - df[\"time_start\"]).clip(lower=1)\n",
        "\n",
        "df = df.sort_values([\"user_id\", \"time_start\"]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37ac69b",
      "metadata": {
        "id": "a37ac69b"
      },
      "source": [
        "## Label construction: revisit within a fixed time window\n",
        "\n",
        "In this part we create the binary target label.  \n",
        "For each user–streamer pair, we look at the next interaction with the same streamer and measure the time gap.\n",
        "\n",
        "If the next interaction occurs within `PREDICTION_WINDOW_STEPS`, we assign label = 1.  \n",
        "Otherwise label = 0.\n",
        "\n",
        "We do this because the modeling objective is to identify near-future revisits rather than long-term interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b8f8d4",
      "metadata": {
        "id": "49b8f8d4"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values([\"user_id\", \"streamer\", \"time_start\"]).reset_index(drop=True)\n",
        "\n",
        "next_start = df.groupby([\"user_id\", \"streamer\"])[\"time_start\"].shift(-1)\n",
        "delta = next_start - df[\"time_start\"]\n",
        "df[\"label\"] = ((delta <= PREDICTION_WINDOW_STEPS) & delta.notna()).astype(int)\n",
        "\n",
        "df = df.sort_values([\"time_start\", \"user_id\"]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abfde94",
      "metadata": {
        "id": "3abfde94"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "In this part we engineer features capturing user–streamer history, overall user behavior, streamer characteristics, similarity signals, and temporal dynamics.\n",
        "\n",
        "We do this because raw IDs cannot be used directly by logistic regression, and these behavioral features encode meaningful signals relevant to revisit likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6a6a6b",
      "metadata": {
        "id": "fa6a6a6b"
      },
      "source": [
        "### User–streamer historical features\n",
        "\n",
        "These features describe how much the user has previously interacted with this streamer.\n",
        "\n",
        "We compute:\n",
        "- frequency of past interactions,\n",
        "- cumulative duration with the streamer,\n",
        "- average duration per interaction,\n",
        "- recency since last interaction.\n",
        "\n",
        "We do this because stronger and more recent user–streamer relationships increase revisit probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8863f624",
      "metadata": {
        "id": "8863f624"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values([\"user_id\", \"streamer\", \"time_start\"]).reset_index(drop=True)\n",
        "\n",
        "df[\"freq_user_streamer\"] = df.groupby([\"user_id\", \"streamer\"]).cumcount()\n",
        "\n",
        "cumdur = df.groupby([\"user_id\", \"streamer\"])[\"duration_steps\"].cumsum()\n",
        "df[\"totaldur_user_streamer\"] = cumdur - df[\"duration_steps\"]\n",
        "\n",
        "df[\"avgdur_user_streamer\"] = 0.0\n",
        "mask = df[\"freq_user_streamer\"] > 0\n",
        "df.loc[mask, \"avgdur_user_streamer\"] = df.loc[mask, \"totaldur_user_streamer\"] / df.loc[mask, \"freq_user_streamer\"]\n",
        "\n",
        "prev_start = df.groupby([\"user_id\", \"streamer\"])[\"time_start\"].shift(1)\n",
        "df[\"recency_user_streamer\"] = df[\"time_start\"] - prev_start\n",
        "\n",
        "df[\"recency_user_streamer\"] = df[\"recency_user_streamer\"].fillna(df[\"recency_user_streamer\"].max())\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1532216",
      "metadata": {
        "id": "e1532216"
      },
      "source": [
        "### Global temporal features\n",
        "\n",
        "These features describe activity patterns independent of specific streamers.\n",
        "\n",
        "We compute:\n",
        "- `recency_global_user`: time since the user's previous interaction with any streamer,\n",
        "- `recency_global_streamer`: time since the streamer’s previous interaction from any user.\n",
        "\n",
        "We do this because user activity rhythms and streamer traffic levels influence revisit likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c56bf88",
      "metadata": {
        "id": "1c56bf88"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values([\"user_id\", \"time_start\"])\n",
        "prev_u = df.groupby(\"user_id\")[\"time_start\"].shift(1)\n",
        "df[\"recency_global_user\"] = (df[\"time_start\"] - prev_u).fillna((df[\"time_start\"] - prev_u).max())\n",
        "\n",
        "df = df.sort_values([\"streamer\", \"time_start\"])\n",
        "prev_s = df.groupby(\"streamer\")[\"time_start\"].shift(1)\n",
        "df[\"recency_global_streamer\"] = (df[\"time_start\"] - prev_s).fillna((df[\"time_start\"] - prev_s).max())\n",
        "\n",
        "df = df.sort_values([\"user_id\", \"time_start\"]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db08395c",
      "metadata": {
        "id": "db08395c"
      },
      "source": [
        "### User breadth and session index\n",
        "\n",
        "Here we capture the user’s exploration behavior and their position within their interaction timeline.\n",
        "\n",
        "We compute:\n",
        "- number of distinct streamers so far,\n",
        "- session index,\n",
        "- log-transformed session index.\n",
        "\n",
        "We do this because users who explore widely behave differently from users who stay focused, and early vs. late session behaviors differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e11f7b8",
      "metadata": {
        "id": "7e11f7b8"
      },
      "outputs": [],
      "source": [
        "def cum_nunique(vals):\n",
        "    s = set()\n",
        "    out = []\n",
        "    for v in vals:\n",
        "        out.append(len(s))\n",
        "        s.add(v)\n",
        "    return pd.Series(out, index=vals.index)\n",
        "\n",
        "df[\"num_distinct_streamers_user\"] = df.groupby(\"user_id\")[\"streamer\"].transform(cum_nunique)\n",
        "df[\"session_idx\"] = df.groupby(\"user_id\").cumcount()\n",
        "df[\"log_session_idx\"] = np.log1p(df[\"session_idx\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f24bea",
      "metadata": {
        "id": "07f24bea"
      },
      "source": [
        "### Streamer global statistics\n",
        "\n",
        "These describe long-term streamer popularity patterns.\n",
        "\n",
        "We compute:\n",
        "- event count,\n",
        "- number of unique users,\n",
        "- total duration,\n",
        "- average duration.\n",
        "\n",
        "We do this because popular streamers and niche streamers attract very different revisit dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6182db",
      "metadata": {
        "id": "2a6182db"
      },
      "outputs": [],
      "source": [
        "streamer_counts = df.groupby(\"streamer\")[\"user_id\"].count().rename(\"streamer_event_count\")\n",
        "streamer_users = df.groupby(\"streamer\")[\"user_id\"].nunique().rename(\"streamer_unique_users\")\n",
        "streamer_dur = df.groupby(\"streamer\")[\"duration_steps\"].sum().rename(\"streamer_total_duration\")\n",
        "\n",
        "stats = pd.concat([streamer_counts, streamer_users, streamer_dur], axis=1)\n",
        "stats[\"streamer_avg_duration\"] = stats[\"streamer_total_duration\"] / stats[\"streamer_event_count\"]\n",
        "\n",
        "df = df.merge(stats, on=\"streamer\", how=\"left\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b69430",
      "metadata": {
        "id": "50b69430"
      },
      "source": [
        "### Item–item co-visitation similarity\n",
        "\n",
        "Here we compute similarity between streamers by counting how often the same user watches both.\n",
        "\n",
        "We do this because collaborative filtering signals help capture user taste patterns not visible from single-streamer history alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1e7bc2",
      "metadata": {
        "id": "8b1e7bc2"
      },
      "outputs": [],
      "source": [
        "items_per_user = df.groupby(\"user_id\")[\"streamer\"].apply(lambda x: sorted(x.unique()))\n",
        "item_freq = df.groupby(\"streamer\")[\"user_id\"].nunique()\n",
        "\n",
        "pair_count = defaultdict(int)\n",
        "\n",
        "for items in items_per_user:\n",
        "    n = len(items)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            a, b = items[i], items[j]\n",
        "            pair_count[(a, b)] += 1\n",
        "            pair_count[(b, a)] += 1\n",
        "\n",
        "item_sim = { (a, b): c / np.sqrt(item_freq[a] * item_freq[b]) for (a, b), c in pair_count.items() }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fede541",
      "metadata": {
        "id": "0fede541"
      },
      "source": [
        "### Item similarity feature\n",
        "\n",
        "For each interaction, we compute the average similarity between the current streamer and all streamers the user has watched before.\n",
        "\n",
        "We do this because users tend to revisit streamers similar to their previous preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e057614b",
      "metadata": {
        "id": "e057614b"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values([\"user_id\", \"time_start\"])\n",
        "df[\"item_sim_mean\"] = 0.0\n",
        "\n",
        "user_hist = defaultdict(set)\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    u = row.user_id\n",
        "    s = row.streamer\n",
        "    others = user_hist[u] - {s}\n",
        "    if others:\n",
        "        sims = [item_sim.get((s, o), 0.0) for o in others]\n",
        "        df.at[idx, \"item_sim_mean\"] = float(np.mean(sims))\n",
        "    user_hist[u].add(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92afb276",
      "metadata": {
        "id": "92afb276"
      },
      "source": [
        "### Share-of-duration feature\n",
        "\n",
        "This feature describes how much of the user’s historical watch time belonged to this streamer.\n",
        "\n",
        "We do this because a high share indicates strong preference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1939c2f4",
      "metadata": {
        "id": "1939c2f4"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values([\"user_id\", \"time_start\"]).reset_index(drop=True)\n",
        "\n",
        "user_cum = df.groupby(\"user_id\")[\"duration_steps\"].cumsum()\n",
        "df[\"user_total_before\"] = user_cum - df[\"duration_steps\"]\n",
        "\n",
        "df[\"share_dur_streamer_in_user\"] = 0.0\n",
        "mask = df[\"user_total_before\"] > 0\n",
        "df.loc[mask, \"share_dur_streamer_in_user\"] = df.loc[mask, \"totaldur_user_streamer\"] / df.loc[mask, \"user_total_before\"]\n",
        "\n",
        "df[\"share_dur_streamer_in_user\"] = df[\"share_dur_streamer_in_user\"].fillna(0.0)\n",
        "df = df.drop(columns=[\"user_total_before\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e32a6770",
      "metadata": {
        "id": "e32a6770"
      },
      "source": [
        "### Log-transform skewed features\n",
        "\n",
        "We apply log transforms to several highly skewed count/duration features to stabilize scale for logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfeaa5ee",
      "metadata": {
        "id": "cfeaa5ee"
      },
      "outputs": [],
      "source": [
        "skew_cols = [\n",
        "    \"freq_user_streamer\",\n",
        "    \"totaldur_user_streamer\",\n",
        "    \"recency_user_streamer\",\n",
        "    \"recency_global_user\",\n",
        "    \"recency_global_streamer\",\n",
        "    \"streamer_event_count\",\n",
        "    \"streamer_unique_users\",\n",
        "    \"streamer_total_duration\"\n",
        "]\n",
        "\n",
        "for c in skew_cols:\n",
        "    df[f\"log_{c}\"] = np.log1p(df[c])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97e990c",
      "metadata": {
        "id": "a97e990c"
      },
      "source": [
        "## Train–test split\n",
        "\n",
        "We sort by time and assign the earliest part of the dataset to training and the most recent part to testing.\n",
        "\n",
        "We do this because temporal prediction must not leak future information into training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a111d4",
      "metadata": {
        "id": "b5a111d4"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(\"time_start\").reset_index(drop=True)\n",
        "n = len(df)\n",
        "n_test = int(TEST_FRACTION * n)\n",
        "\n",
        "train_df = df.iloc[:-n_test].copy()\n",
        "test_df = df.iloc[-n_test:].copy()\n",
        "\n",
        "y_train = train_df[\"label\"].values\n",
        "y_test = test_df[\"label\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f19425",
      "metadata": {
        "id": "89f19425"
      },
      "source": [
        "## Feature matrix and scaling\n",
        "\n",
        "We standardize features because logistic regression performs better when inputs share a common scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c63247",
      "metadata": {
        "id": "58c63247"
      },
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    \"log_freq_user_streamer\",\n",
        "    \"log_totaldur_user_streamer\",\n",
        "    \"avgdur_user_streamer\",\n",
        "    \"log_recency_user_streamer\",\n",
        "    \"share_dur_streamer_in_user\",\n",
        "    \"num_distinct_streamers_user\",\n",
        "    \"log_streamer_event_count\",\n",
        "    \"log_streamer_unique_users\",\n",
        "    \"log_streamer_total_duration\",\n",
        "    \"streamer_avg_duration\",\n",
        "    \"log_recency_global_user\",\n",
        "    \"log_recency_global_streamer\",\n",
        "    \"item_sim_mean\",\n",
        "    \"log_session_idx\"\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train_df[feature_cols])\n",
        "X_test = scaler.transform(test_df[feature_cols])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "safe_cols = [\n",
        "    \"freq_user_streamer\",\n",
        "    \"totaldur_user_streamer\",\n",
        "    \"recency_user_streamer\",\n",
        "    \"recency_global_user\",\n",
        "    \"recency_global_streamer\",\n",
        "    \"streamer_event_count\",\n",
        "    \"streamer_unique_users\",\n",
        "    \"streamer_total_duration\"\n",
        "]\n",
        "\n",
        "for c in safe_cols:\n",
        "    train_df[c] = train_df[c].clip(lower=0)\n",
        "    test_df[c] = test_df[c].clip(lower=0)\n",
        "\n",
        "train_df[feature_cols] = train_df[feature_cols].fillna(0)\n",
        "test_df[feature_cols] = test_df[feature_cols].fillna(0)\n",
        "\n",
        "print(\"Train NaNs:\", train_df[feature_cols].isna().sum().sum())\n",
        "print(\"Test NaNs:\", test_df[feature_cols].isna().sum().sum())"
      ],
      "metadata": {
        "id": "MXf7hm8Xu705"
      },
      "id": "MXf7hm8Xu705",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "13c3df93",
      "metadata": {
        "id": "13c3df93"
      },
      "source": [
        "## Baseline models\n",
        "\n",
        "We include trivial baselines to contextualize the performance of learned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfab3fb",
      "metadata": {
        "id": "6dfab3fb"
      },
      "outputs": [],
      "source": [
        "random_scores = np.random.rand(len(test_df))\n",
        "\n",
        "streamer_pop_train = train_df[\"streamer\"].value_counts()\n",
        "pop_scores = test_df[\"streamer\"].map(streamer_pop_train).fillna(0).values\n",
        "\n",
        "user_streamer_counts = train_df.groupby([\"user_id\", \"streamer\"]).size()\n",
        "\n",
        "def user_top_score(row):\n",
        "    return user_streamer_counts.get((row.user_id, row.streamer), 0)\n",
        "\n",
        "user_top_scores = test_df.apply(user_top_score, axis=1).values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e77324",
      "metadata": {
        "id": "93e77324"
      },
      "source": [
        "## Evaluation helpers\n",
        "\n",
        "We compute both threshold-based evaluation metrics and ranking metrics such as precision@K and recall@K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a358748d",
      "metadata": {
        "id": "a358748d"
      },
      "outputs": [],
      "source": [
        "def evaluate_basic(y_true, scores, th=0.5):\n",
        "    preds = (scores >= th).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, preds),\n",
        "        \"roc_auc\": roc_auc_score(y_true, scores),\n",
        "        \"precision\": precision_score(y_true, preds, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, preds, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, preds, zero_division=0)\n",
        "    }\n",
        "\n",
        "def evaluate_at_k(test_df, scores, k=10):\n",
        "    df2 = test_df.copy()\n",
        "    df2[\"score\"] = scores\n",
        "    prec = []\n",
        "    rec = []\n",
        "    for u, g in df2.groupby(\"user_id\"):\n",
        "        g = g.sort_values(\"score\", ascending=False)\n",
        "        topk = g.head(k)\n",
        "        hits = topk[\"label\"].sum()\n",
        "        total = g[\"label\"].sum()\n",
        "        if total > 0:\n",
        "            prec.append(hits / k)\n",
        "            rec.append(hits / total)\n",
        "    return {f\"precision@{k}\": np.nanmean(prec), f\"recall@{k}\": np.nanmean(rec)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6755aef4",
      "metadata": {
        "id": "6755aef4"
      },
      "source": [
        "## Logistic Regression with hyperparameter tuning\n",
        "\n",
        "We tune over penalty types and regularization strengths to find the model maximizing ROC AUC.\n",
        "\n",
        "We do this because logistic regression is sensitive to its regularization settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595f1505",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "595f1505",
        "outputId": "2ffba23b-f8b5-4f99-97ba-b8198602495a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-161991565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             w0, n_iter_i, warm_start_sag = sag_solver(\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0msag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msag64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msag32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     num_seen, n_iter_ = sag(\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "C_values = [0.01, 0.1, 1.0, 3.0, 10.0]\n",
        "penalties = [\"l1\", \"l2\"]\n",
        "\n",
        "best_lr_auc = -1\n",
        "best_lr_model = None\n",
        "best_lr_params = None\n",
        "\n",
        "for C in C_values:\n",
        "    for pen in penalties:\n",
        "        try:\n",
        "            model = LogisticRegression(\n",
        "                max_iter=4000,\n",
        "                class_weight=\"balanced\",\n",
        "                solver=\"saga\",\n",
        "                penalty=pen,\n",
        "                C=C,\n",
        "                random_state=RANDOM_STATE\n",
        "            )\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            preds = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "            if auc > best_lr_auc:\n",
        "                best_lr_auc = auc\n",
        "                best_lr_model = model\n",
        "                best_lr_params = (C, pen)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for C={C}, penalty={pen}: {e}\")\n",
        "\n",
        "# Safety check\n",
        "if best_lr_model is None:\n",
        "    raise RuntimeError(\"No logistic regression model successfully trained.\")\n",
        "\n",
        "# Final best model\n",
        "logreg = best_lr_model\n",
        "logit_scores = logreg.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958c59f8",
      "metadata": {
        "id": "958c59f8"
      },
      "source": [
        "## Collaborative filtering scoring\n",
        "\n",
        "We compute CF scores by averaging similarity to all streamers a user has watched before.\n",
        "\n",
        "We do this because CF captures shared user taste patterns absent from explicit features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b18265",
      "metadata": {
        "id": "14b18265"
      },
      "outputs": [],
      "source": [
        "cf_scores = []\n",
        "user_history_train = defaultdict(list)\n",
        "\n",
        "for _, row in train_df.iterrows():\n",
        "    user_history_train[row.user_id].append(row.streamer)\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    u = row.user_id\n",
        "    s = row.streamer\n",
        "    hist = user_history_train.get(u, [])\n",
        "    if not hist:\n",
        "        cf_scores.append(0.0)\n",
        "        continue\n",
        "    sims = [item_sim.get((s, h), 0.0) for h in set(hist)]\n",
        "    cf_scores.append(np.mean(sims) if sims else 0.0)\n",
        "\n",
        "cf_scores = np.array(cf_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716cf6a0",
      "metadata": {
        "id": "716cf6a0"
      },
      "source": [
        "## Hybrid model\n",
        "\n",
        "We combine logistic regression and collaborative filtering via weighted interpolation.\n",
        "\n",
        "We do this because the two models capture complementary signals: LR uses behavioral features while CF uses similarity structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae9e4b0",
      "metadata": {
        "id": "4ae9e4b0"
      },
      "outputs": [],
      "source": [
        "alphas = np.linspace(0, 1, 21)\n",
        "best_alpha = 0.5\n",
        "best_alpha_auc = -1\n",
        "\n",
        "for a in alphas:\n",
        "    hyb = a * logit_scores + (1 - a) * cf_scores\n",
        "    auc = roc_auc_score(y_test, hyb)\n",
        "    if auc > best_alpha_auc:\n",
        "        best_alpha_auc = auc\n",
        "        best_alpha = a\n",
        "\n",
        "hyb_scores = best_alpha * logit_scores + (1 - best_alpha) * cf_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93adf60",
      "metadata": {
        "id": "a93adf60"
      },
      "source": [
        "## Threshold tuning\n",
        "\n",
        "We select the probability threshold that maximizes F1 score.\n",
        "\n",
        "We do this because optimal thresholds vary depending on metric choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbdc42e",
      "metadata": {
        "id": "9fbdc42e"
      },
      "outputs": [],
      "source": [
        "thresholds = np.linspace(0.01, 0.5, 50)\n",
        "best_f1 = -1\n",
        "best_th = 0.5\n",
        "\n",
        "for th in thresholds:\n",
        "    metrics = evaluate_basic(y_test, logit_scores, th=th)\n",
        "    if metrics[\"f1\"] > best_f1:\n",
        "        best_f1 = metrics[\"f1\"]\n",
        "        best_th = th"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02233281",
      "metadata": {
        "id": "02233281"
      },
      "source": [
        "## Final evaluation\n",
        "\n",
        "We compare logistic regression, collaborative filtering, and the hybrid model using:\n",
        "- accuracy\n",
        "- ROC AUC\n",
        "- precision\n",
        "- recall\n",
        "- F1\n",
        "- precision@K\n",
        "- recall@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260d9b2c",
      "metadata": {
        "id": "260d9b2c"
      },
      "outputs": [],
      "source": [
        "print(\"LogReg:\", evaluate_basic(y_test, logit_scores, th=best_th))\n",
        "print(\"LogReg@K:\", evaluate_at_k(test_df, logit_scores, k=TOP_K))\n",
        "\n",
        "print(\"CF:\", evaluate_basic(y_test, cf_scores, th=0.5))\n",
        "print(\"CF@K:\", evaluate_at_k(test_df, cf_scores, k=TOP_K))\n",
        "\n",
        "print(\"Hybrid:\", evaluate_basic(y_test, hyb_scores, th=best_th))\n",
        "print(\"Hybrid@K:\", evaluate_at_k(test_df, hyb_scores, k=TOP_K))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db35ebc",
      "metadata": {
        "id": "2db35ebc"
      },
      "source": [
        "## Diagnostic plots\n",
        "\n",
        "We visualize ROC, precision–recall curves, score distributions, and feature importances to understand model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81a3b55",
      "metadata": {
        "id": "f81a3b55"
      },
      "outputs": [],
      "source": [
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, logit_scores)\n",
        "fpr_h, tpr_h, _ = roc_curve(y_test, hyb_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_lr, tpr_lr)\n",
        "plt.plot(fpr_h, tpr_h)\n",
        "plt.plot([0,1],[0,1],\"--\")\n",
        "plt.show()\n",
        "\n",
        "prec_lr, rec_lr, _ = precision_recall_curve(y_test, logit_scores)\n",
        "prec_h, rec_h, _ = precision_recall_curve(y_test, hyb_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rec_lr, prec_lr)\n",
        "plt.plot(rec_h, prec_h)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(logit_scores[y_test == 0], bins=50, alpha=0.5)\n",
        "plt.hist(logit_scores[y_test == 1], bins=50, alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "coef = logreg.coef_[0]\n",
        "feat_importance = pd.Series(coef, index=feature_cols).sort_values(key=lambda x: abs(x), ascending=False)\n",
        "feat_importance.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c1a5e0",
      "metadata": {
        "id": "46c1a5e0"
      },
      "source": [
        "## Failure case analysis\n",
        "\n",
        "We inspect top false positives and false negatives to understand where the model struggles.  \n",
        "This helps identify systematic issues not visible from metrics alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb536df8",
      "metadata": {
        "id": "cb536df8"
      },
      "outputs": [],
      "source": [
        "eval_df = test_df.copy()\n",
        "eval_df[\"lr_score\"] = logit_scores\n",
        "\n",
        "false_positives = eval_df[eval_df[\"label\"] == 0].sort_values(\"lr_score\", ascending=False).head(10)\n",
        "false_negatives = eval_df[eval_df[\"label\"] == 1].sort_values(\"lr_score\", ascending=True).head(10)\n",
        "\n",
        "display(false_positives[[\"user_id\",\"streamer\",\"time_start\",\"label\",\"lr_score\"]])\n",
        "display(false_negatives[[\"user_id\",\"streamer\",\"time_start\",\"label\",\"lr_score\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b1f762",
      "metadata": {
        "id": "c8b1f762"
      },
      "source": [
        "## Save final model artifacts\n",
        "\n",
        "We save the trained logistic regression model, scaler, feature list, and hybrid mixing parameter.\n",
        "\n",
        "We do this because downstream inference requires consistent preprocessing and model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64504679",
      "metadata": {
        "id": "64504679"
      },
      "outputs": [],
      "source": [
        "artifacts = {\n",
        "    \"logreg\": logreg,\n",
        "    \"scaler\": scaler,\n",
        "    \"feature_cols\": feature_cols,\n",
        "    \"best_alpha\": best_alpha,\n",
        "    \"best_threshold\": best_th\n",
        "}\n",
        "\n",
        "with open(\"part2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(artifacts, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a18156",
      "metadata": {
        "id": "48a18156"
      },
      "source": [
        "Part 2 All Done"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}